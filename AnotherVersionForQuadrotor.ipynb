{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 三星级复现项目：使用DDPG解决四轴飞行器速度控制\n",
    "（这可能是史上最“偷懒”的三星级复现项目，改改任务环境就可以提交了 - -！应该没有更懒的了，O(∩_∩)O哈哈~）\n",
    "\n",
    "# Step1 安装依赖"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "!pip uninstall -y parl  # 说明：AIStudio预装的parl版本太老，容易跟其他库产生兼容性冲突，建议先卸载\r\n",
    "!pip uninstall -y pandas scikit-learn # 提示：在AIStudio中卸载这两个库再import parl可避免warning提示，不卸载也不影响parl的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling parl-1.1.2:\n",
      "  Successfully uninstalled parl-1.1.2\n",
      "Uninstalling pandas-0.23.4:\n",
      "  Successfully uninstalled pandas-0.23.4\n",
      "Uninstalling scikit-learn-0.20.0:\n",
      "  Successfully uninstalled scikit-learn-0.20.0\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple\n",
      "Collecting paddlepaddle==1.6.3\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/96/28/e72bebb3c9b3d98eb9b15d9f6d85150f3cbd63e695e59882ff9f04846686/paddlepaddle-1.6.3-cp37-cp37m-manylinux1_x86_64.whl (90.9MB)\n",
      "\u001b[K     |████████████████████████████████| 90.9MB 9.0MB/s eta 0:00:015\n",
      "\u001b[?25hRequirement already satisfied: scipy; python_version >= \"3.5\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (1.3.0)\n",
      "Requirement already satisfied: Pillow in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (6.2.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (5.1.2)\n",
      "Requirement already satisfied: rarfile in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (3.1)\n",
      "Requirement already satisfied: matplotlib; python_version >= \"3.6\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (2.2.3)\n",
      "Requirement already satisfied: funcsigs in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (1.0.2)\n",
      "Requirement already satisfied: decorator in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (4.4.0)\n",
      "Requirement already satisfied: nltk; python_version >= \"3.5\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (3.4.5)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (1.12.0)\n",
      "Requirement already satisfied: protobuf>=3.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.12; python_version >= \"3.5\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (1.16.4)\n",
      "Requirement already satisfied: prettytable in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (0.7.2)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (4.1.1.26)\n",
      "Requirement already satisfied: objgraph in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (3.4.1)\n",
      "Requirement already satisfied: requests>=2.20.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (2.22.0)\n",
      "Requirement already satisfied: graphviz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (0.13)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->paddlepaddle==1.6.3) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->paddlepaddle==1.6.3) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->paddlepaddle==1.6.3) (2.8.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->paddlepaddle==1.6.3) (2019.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->paddlepaddle==1.6.3) (0.10.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from protobuf>=3.1.0->paddlepaddle==1.6.3) (41.4.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle==1.6.3) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle==1.6.3) (1.25.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle==1.6.3) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle==1.6.3) (3.0.4)\n",
      "Installing collected packages: paddlepaddle\n",
      "  Found existing installation: paddlepaddle 1.6.2\n",
      "    Uninstalling paddlepaddle-1.6.2:\n",
      "      Successfully uninstalled paddlepaddle-1.6.2\n",
      "Successfully installed paddlepaddle-1.6.3\n",
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Collecting parl==1.3.1\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/62/79/590af38a920792c71afb73fad7583967928b4d0ba9fca76250d935c7fda8/parl-1.3.1-py2.py3-none-any.whl (521kB)\n",
      "\u001b[K     |████████████████████████████████| 522kB 8.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: flask>=1.0.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (1.1.1)\n",
      "Collecting psutil>=5.6.2 (from parl==1.3.1)\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/c4/b8/3512f0e93e0db23a71d82485ba256071ebef99b227351f0f5540f744af41/psutil-5.7.0.tar.gz (449kB)\n",
      "\u001b[K     |████████████████████████████████| 450kB 9.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tb-nightly==1.15.0a20190801 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (1.15.0a20190801)\n",
      "Collecting flask-cors (from parl==1.3.1)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
      "Collecting visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\" (from parl==1.3.1)\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/65/b8/88c942a60157e544b663729f66ffce8789edaa4d7df2a5ed6b2f39f93609/visualdl-2.0.0b7-py3-none-any.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 92kB/s eta 0:00:015\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (1.3.0)\n",
      "Requirement already satisfied: cloudpickle==1.2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (1.2.1)\n",
      "Requirement already satisfied: tensorboardX==1.8 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (1.8)\n",
      "Requirement already satisfied: click in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (7.0)\n",
      "Requirement already satisfied: pyarrow==0.13.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (0.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (1.1.0)\n",
      "Requirement already satisfied: pyzmq==18.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (18.0.1)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.0.4->parl==1.3.1) (1.1.0)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.0.4->parl==1.3.1) (0.16.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.0.4->parl==1.3.1) (2.10.3)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tb-nightly==1.15.0a20190801->parl==1.3.1) (1.16.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tb-nightly==1.15.0a20190801->parl==1.3.1) (0.8.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tb-nightly==1.15.0a20190801->parl==1.3.1) (0.33.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tb-nightly==1.15.0a20190801->parl==1.3.1) (41.4.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tb-nightly==1.15.0a20190801->parl==1.3.1) (1.12.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tb-nightly==1.15.0a20190801->parl==1.3.1) (3.1.1)\n",
      "Requirement already satisfied: grpcio>=1.6.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tb-nightly==1.15.0a20190801->parl==1.3.1) (1.26.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tb-nightly==1.15.0a20190801->parl==1.3.1) (3.10.0)\n",
      "Collecting Pillow>=7.0.0 (from visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1)\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/ab/f8/d3627cc230270a6a4eedee32974fbc8cb26c5fdb8710dd5ea70133640022/Pillow-7.1.2-cp37-cp37m-manylinux1_x86_64.whl (2.1MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1MB 35.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (1.21.0)\n",
      "Requirement already satisfied: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (3.7.9)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (2.22.0)\n",
      "Collecting Flask-Babel>=1.0.0 (from visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/76/a4/0115c7c520125853037fc1d6b3da132a526949640e27a699a13e05ec7593/Flask_Babel-1.0.0-py3-none-any.whl\n",
      "Collecting hdfs (from visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1)\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/82/39/2c0879b1bcfd1f6ad078eb210d09dbce21072386a3997074ee91e60ddc5a/hdfs-2.5.8.tar.gz (41kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 32.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: opencv-python in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (4.1.1.26)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.0.4->parl==1.3.1) (1.1.1)\n",
      "Requirement already satisfied: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (1.4.10)\n",
      "Requirement already satisfied: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (0.10.0)\n",
      "Requirement already satisfied: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (1.3.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (5.1.2)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (0.23)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (2.0.1)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (1.3.4)\n",
      "Requirement already satisfied: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (16.7.9)\n",
      "Requirement already satisfied: pycodestyle<2.6.0,>=2.5.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (2.5.0)\n",
      "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (0.6.1)\n",
      "Requirement already satisfied: pyflakes<2.2.0,>=2.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (2.1.1)\n",
      "Requirement already satisfied: entrypoints<0.4.0,>=0.3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (1.25.6)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (2.8)\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (2019.3)\n",
      "Collecting Babel>=2.3 (from Flask-Babel>=1.0.0->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1)\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/15/a1/522dccd23e5d2e47aed4b6a16795b8213e3272c7506e625f2425ad025a19/Babel-2.8.0-py2.py3-none-any.whl (8.6MB)\n",
      "\u001b[K     |████████████████████████████████| 8.6MB 455kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting docopt (from hdfs->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/a2/55/8f8cab2afd404cf578136ef2cc5dfb50baa1761b68c9da1fb1e4eed343c9/docopt-0.6.2.tar.gz\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (7.2.0)\n",
      "Building wheels for collected packages: psutil, hdfs, docopt\n",
      "  Building wheel for psutil (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for psutil: filename=psutil-5.7.0-cp37-cp37m-linux_x86_64.whl size=261255 sha256=81c5186c9c2437096f9350ec4ad0e5a9f03f30af81bd6ea80496dce5bc6eb9bf\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/fd/8b/7e/939524c38be886652fe8b1688384da4bafe0a8224d504e90eb\n",
      "  Building wheel for hdfs (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdfs: filename=hdfs-2.5.8-cp37-none-any.whl size=33214 sha256=91de0c9052db30ff5d703253b11f7ce42e4be974bddd4d6c8875368a1638f087\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/35/91/05/ed325f80520cc72b4eaa7327f96358c62d84afd098625ed2bd\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=89bd158f1d69930f15af1672a5b24fd99c45a6fcb9e9849bb83d2c27db67ed80\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/78/da/5a/be54433e626178926da00dbc53e06294ba87ec2c37dded83b4\n",
      "Successfully built psutil hdfs docopt\n",
      "\u001b[31mERROR: paddlehub 1.6.0 requires pandas; python_version >= \"3\", which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: visualdl 2.0.0b7 has requirement protobuf>=3.11.0, but you'll have protobuf 3.10.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: visualdl 2.0.0b7 has requirement six>=1.14.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: psutil, flask-cors, Pillow, Babel, Flask-Babel, docopt, hdfs, visualdl, parl\n",
      "  Found existing installation: Pillow 6.2.0\n",
      "    Uninstalling Pillow-6.2.0:\n",
      "      Successfully uninstalled Pillow-6.2.0\n",
      "  Found existing installation: visualdl 1.3.0\n",
      "    Uninstalling visualdl-1.3.0:\n",
      "      Successfully uninstalled visualdl-1.3.0\n",
      "Successfully installed Babel-2.8.0 Flask-Babel-1.0.0 Pillow-7.1.2 docopt-0.6.2 flask-cors-3.0.8 hdfs-2.5.8 parl-1.3.1 psutil-5.7.0 visualdl-2.0.0b7\n",
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Collecting rlschool==0.3.1\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/45/8b/a6885b8815e364f21a37e0580d1018871a8aab882586e87d60f1f6a1e55a/rlschool-0.3.1-py3-none-any.whl (461kB)\n",
      "\u001b[K     |████████████████████████████████| 471kB 6.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.12.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from rlschool==0.3.1) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.16.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from rlschool==0.3.1) (1.16.4)\n",
      "Collecting colour>=0.1.5 (from rlschool==0.3.1)\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.mirrors.ustc.edu.cn', port=443): Read timed out. (read timeout=15)\")': /simple/colour/\u001b[0m\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/74/46/e81907704ab203206769dee1385dc77e1407576ff8f50a0681d0a6b541be/colour-0.1.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: networkx>=2.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from rlschool==0.3.1) (2.4)\n",
      "Requirement already satisfied: Pillow>=6.2.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from rlschool==0.3.1) (7.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from rlschool==0.3.1) (1.12.0)\n",
      "Collecting pyglet==1.5.0; python_version >= \"3\" (from rlschool==0.3.1)\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/70/ca/20aee170afe6011e295e34b27ad7d7ccd795faba581dd3c6f7cec237f561/pyglet-1.5.0-py2.py3-none-any.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 22.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting configparser>=3.7.4 (from rlschool==0.3.1)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\n",
      "Collecting trimesh>=3.2.39 (from rlschool==0.3.1)\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/0a/2d/071cbfed35607e42b732d7135cfb7bb4ca9ee6b64a9bd16888e50cf52eac/trimesh-3.7.4-py3-none-any.whl (615kB)\n",
      "\u001b[K     |████████████████████████████████| 624kB 31.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from networkx>=2.2->rlschool==0.3.1) (4.4.0)\n",
      "Requirement already satisfied: future in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pyglet==1.5.0; python_version >= \"3\"->rlschool==0.3.1) (0.18.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from trimesh>=3.2.39->rlschool==0.3.1) (41.4.0)\n",
      "Installing collected packages: colour, pyglet, configparser, trimesh, rlschool\n",
      "  Found existing installation: pyglet 1.4.5\n",
      "    Uninstalling pyglet-1.4.5:\n",
      "      Successfully uninstalled pyglet-1.4.5\n",
      "Successfully installed colour-0.1.5 configparser-5.0.0 pyglet-1.5.0 rlschool-0.3.1 trimesh-3.7.4\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y parl  # 说明：AIStudio预装的parl版本太老，容易跟其他库产生兼容性冲突，建议先卸载\n",
    "!pip uninstall -y pandas scikit-learn # 提示：在AIStudio中卸载这两个库再import parl可避免warning提示，不卸载也不影响parl的使用\n",
    "\n",
    "!pip install paddlepaddle==1.6.3  -i https://mirror.baidu.com/pypi/simple           #可选安装paddlepaddle-gpu==1.6.3.post97\n",
    "!pip install parl==1.3.1\n",
    "!pip install rlschool==0.3.1\n",
    "\n",
    "# 说明：安装日志中出现两条红色的关于 paddlehub 和 visualdl 的 ERROR 与parl无关，可以忽略，不影响使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paddlepaddle         1.6.3          \n",
      "parl                 1.3.1          \n",
      "rlschool             0.3.1          \n"
     ]
    }
   ],
   "source": [
    "# 检查依赖包版本是否正确\n",
    "!pip list | grep paddlepaddle\n",
    "!pip list | grep parl\n",
    "!pip list | grep rlschool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Step2 导入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import parl\n",
    "from parl import layers\n",
    "from paddle import fluid\n",
    "from parl.utils import logger\n",
    "from parl.utils import action_mapping # 将神经网络输出映射到对应的 实际动作取值范围 内\n",
    "from parl.utils import ReplayMemory # 经验回放\n",
    "\n",
    "from rlschool import make_env  # 使用 RLSchool 创建飞行器环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Step3 设置超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "######################################################################\n",
    "#\n",
    "# 1. 请设定 learning rate，尝试增减查看效果\n",
    "#\n",
    "######################################################################\n",
    "######################################################################\n",
    "ACTOR_LR =5* 0.0002   # Actor网络更新的 learning rate                开始直接5倍学习率，后期模型相对稳定后再调低\n",
    "CRITIC_LR =5* 0.001   # Critic网络更新的 learning rate                开始直接5倍学习率，后期模型相对稳定后再调低\n",
    "\n",
    "GAMMA = 0.99        # reward 的衰减因子，一般取 0.9 到 0.999 不等\n",
    "TAU = 0.001         # target_model 跟 model 同步参数 的 软更新参数\n",
    "MEMORY_SIZE = 60e4   # replay memory的大小，越大越占用内存\n",
    "MEMORY_WARMUP_SIZE = 1e4      # replay_memory 里需要预存一些经验数据，再从里面sample一个batch的经验让agent去learn            \n",
    "REWARD_SCALE = 0.01       # reward 的缩放因子\n",
    "BATCH_SIZE = 2*256          # 每次给agent learn的数据数量，从replay memory随机里sample一批数据出来                 2倍的batch_size \n",
    "TRAIN_TOTAL_STEPS = 60e4   # 总训练步数\n",
    "TEST_EVERY_STEPS = 1e4    # 每个N步评估一下算法效果，每次评估5个episode求平均reward\n",
    "GM  = 0.2                 # 变电压的浮动参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Step4 搭建Model、Algorithm、Agent架构\n",
    "* `Agent`把产生的数据传给`algorithm`，`algorithm`根据`model`的模型结构计算出`Loss`，使用`SGD`或者其他优化器不断的优化，`PARL`这种架构可以很方便的应用在各类深度强化学习问题中。\n",
    "\n",
    "## （1）Model\n",
    "* 分别搭建`Actor`、`Critic`的`Model`结构，构建`QuadrotorModel`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ActorModel(parl.Model):\n",
    "    def __init__(self, act_dim):\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        #\n",
    "        # 2. 请配置model结构\n",
    "        #\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        hid_size1 = 64\n",
    "        hid_size2 = 64\n",
    "\n",
    "        self.fc1 = layers.fc(size=hid_size1, act='relu',param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "        self.fc2 = layers.fc(size=hid_size2, act='relu',param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "        self.fc3 = layers.fc(size=act_dim  , act='tanh',param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "\n",
    "    def policy(self, obs):\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        #\n",
    "        # 3. 请组装policy网络\n",
    "        #\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        hid = self.fc1(obs)\n",
    "        hid = self.fc2(hid)\n",
    "        logits = self.fc3(hid)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CriticModel(parl.Model):\n",
    "    def __init__(self):\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        #\n",
    "        # 4. 请配置model结构\n",
    "        #\n",
    "        ######################################################################               \n",
    "        ######################################################################                \n",
    "        hid_size = 100\n",
    "\n",
    "        self.fc1 = layers.fc(size=hid_size, act='relu',param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "        self.fc2 = layers.fc(size=1, act=None)\n",
    "\n",
    "    def value(self, obs, act):\n",
    "        # 输入 state, action, 输出对应的Q(s,a)\n",
    "\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        #\n",
    "        # 5. 请组装Q网络\n",
    "        #\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        concat = layers.concat([obs, act], axis=1)\n",
    "        hid = self.fc1(concat)\n",
    "        Q = self.fc2(hid)\n",
    "        Q = layers.squeeze(Q, axes=[1])\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QuadrotorModel(parl.Model):\n",
    "    def __init__(self, act_dim):\n",
    "        self.actor_model = ActorModel(act_dim)\n",
    "        self.critic_model = CriticModel()\n",
    "\n",
    "    def policy(self, obs):\n",
    "        return self.actor_model.policy(obs)\n",
    "\n",
    "    def value(self, obs, act):\n",
    "        return self.critic_model.value(obs, act)\n",
    "\n",
    "    def get_actor_params(self):\n",
    "        return self.actor_model.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## （2）Algorithm\n",
    "* 可以采用下面的方式从`parl`库中快速引入`DDPG`算法，无需自己重新写算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from parl.algorithms import DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## （3）Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QuadrotorAgent(parl.Agent):\n",
    "    def __init__(self, algorithm, obs_dim, act_dim):                       \n",
    "        assert isinstance(obs_dim, int)\n",
    "        assert isinstance(act_dim, int)\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        super(QuadrotorAgent, self).__init__(algorithm)\n",
    "\n",
    "        # 注意，在最开始的时候，先完全同步target_model和model的参数\n",
    "        self.alg.sync_target(decay=0)\n",
    "\n",
    "    def build_program(self):\n",
    "        self.pred_program = fluid.Program()\n",
    "        self.learn_program = fluid.Program()\n",
    "\n",
    "        with fluid.program_guard(self.pred_program):\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            self.pred_act = self.alg.predict(obs)\n",
    "\n",
    "        with fluid.program_guard(self.learn_program):\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            act = layers.data(\n",
    "                name='act', shape=[self.act_dim], dtype='float32')\n",
    "            reward = layers.data(name='reward', shape=[], dtype='float32')\n",
    "            next_obs = layers.data(\n",
    "                name='next_obs', shape=[self.obs_dim], dtype='float32')\n",
    "            terminal = layers.data(name='terminal', shape=[], dtype='bool')\n",
    "            _, self.critic_cost = self.alg.learn(obs, act, reward, next_obs,\n",
    "                                                 terminal)\n",
    "\n",
    "    def predict(self, obs):\n",
    "        obs = np.expand_dims(obs, axis=0)\n",
    "        act = self.fluid_executor.run(\n",
    "            self.pred_program, feed={'obs': obs},\n",
    "            fetch_list=[self.pred_act])[0]\n",
    "        return act\n",
    "\n",
    "    def learn(self, obs, act, reward, next_obs, terminal):\n",
    "        feed = {\n",
    "            'obs': obs,\n",
    "            'act': act,\n",
    "            'reward': reward,\n",
    "            'next_obs': next_obs,\n",
    "            'terminal': terminal\n",
    "        }\n",
    "        critic_cost = self.fluid_executor.run(\n",
    "            self.learn_program, feed=feed, fetch_list=[self.critic_cost])[0]\n",
    "        self.alg.sync_target()\n",
    "        return critic_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# Step4 Training && Test（训练&&测试）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_episode(env, agent, rpm):\n",
    "    obs = env.reset()\n",
    "    total_reward, steps = 0, 0\n",
    "    while True:\n",
    "        steps += 1\n",
    "        batch_obs = np.expand_dims(obs, axis=0)\n",
    "        action0 = agent.predict(batch_obs.astype('float32'))            \n",
    "        #action  =    action.mean(axis=1)                #加的一行代码，使输出一致，效果你懂的，值得一试O(∩_∩)O哈哈~\n",
    "        \n",
    "        \n",
    "        action = np.squeeze(action0)                         \n",
    "        mean_a= action[4]                              #加的三行代码，还原输出，目的使输出稳定，相当于加了先验，4轴飞行器的电压的保持相对的稳定，更有利于收敛。       \n",
    "        action = action[0:4]                           #其中一个维度是作为基本值，其他4个维度作为浮动值。\n",
    "        action = GM*action + mean_a                   #此处我取了一个GM = 0.15的系数，为什么有效？可能神经网络训练的时候输出的值是差不多的，强行加一个系数相当于人为的先验。\n",
    "                                             \n",
    "\n",
    "        # 给输出动作增加探索扰动，输出限制在 [-1.0, 1.0] 范围内\n",
    "        action = np.clip(np.random.normal(action, 1.0), -1.0, 1.0)            ## action = np.clip(action, -1.0, 1.0)   ，变成这个样子就是直接用网络输出不加扰动存入经验池，\n",
    "         \n",
    "                                                                              ##大家也可以加大或者降低normal值，来增加或者减小探索的幅度\n",
    "        # 动作映射到对应的 实际动作取值范围 内, action_mapping是从parl.utils那里import进来的函数            \n",
    "        action = action_mapping(action, env.action_space.low[0],\n",
    "                                env.action_space.high[0])\n",
    "       \n",
    "        ##测试print(action)                          #之前测试action用的            \n",
    "\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        rpm.append(obs, action0, REWARD_SCALE * reward, next_obs, done)       #注意变量名 action0，rpm需要原始输出，而env需要处理后的输出\n",
    "\n",
    "        if rpm.size() > MEMORY_WARMUP_SIZE:\n",
    "            batch_obs, batch_action, batch_reward, batch_next_obs, \\\n",
    "                    batch_terminal = rpm.sample_batch(BATCH_SIZE)\n",
    "            critic_cost = agent.learn(batch_obs, batch_action, batch_reward,\n",
    "                                      batch_next_obs, batch_terminal)\n",
    "\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward, steps\n",
    "\n",
    "# 评估 agent, 跑 5 个episode，总reward求平均\n",
    "def evaluate(env, agent):\n",
    "    eval_reward = []\n",
    "    for i in range(5):\n",
    "        obs = env.reset()\n",
    "        total_reward, steps = 0, 0\n",
    "        while True:\n",
    "            batch_obs = np.expand_dims(obs, axis=0)\n",
    "            action = agent.predict(batch_obs.astype('float32'))\n",
    "            ##action[0]  =    action.mean(axis=1)                #加的一行代码，使输出为4个神经元的平均值，此处是之前测试用的，大家也可以试下，\n",
    "                                                                  \n",
    "            action = np.squeeze(action)                      \n",
    "            mean_a= action[4]                                     #加的代码，还原输出，目的使输出稳定，原因同上。\n",
    "            action = action[0:4]\n",
    "            action = GM*action + mean_a                           #此处我取了一个GM = 0.2的系数,在全局变量里面设置，用于变电压浮动的控制\n",
    "\n",
    "            action = np.clip(action, -1.0, 1.0)         #加的一行代码，防止报错\n",
    "            action = action_mapping(action, env.action_space.low[0], \n",
    "                                    env.action_space.high[0])\n",
    "\n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "            obs = next_obs\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        eval_reward.append(total_reward)\n",
    "    return np.mean(eval_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Step 5 创建环境和Agent，创建经验池，启动训练，定期保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06-29 23:48:09 MainThread @machine_info.py:88]\u001b[0m Cannot find available GPU devices, using CPU now.\n",
      "\u001b[32m[06-29 23:48:09 MainThread @machine_info.py:88]\u001b[0m Cannot find available GPU devices, using CPU now.\n",
      "\u001b[32m[06-29 23:48:10 MainThread @machine_info.py:88]\u001b[0m Cannot find available GPU devices, using CPU now.\n"
     ]
    }
   ],
   "source": [
    "# 创建飞行器环境\n",
    "env = make_env(\"Quadrotor\", task=\"velocity_control\", seed=0)              ##关键的点到了，此处为作业到复现最大的改动，就改了一个文件名，说明parl框架的确复用性非常强。\n",
    "env.reset()\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]  +1            #输出加一个维度，评估时再还原\n",
    "\n",
    "\n",
    "# 根据parl框架构建agent\n",
    "######################################################################\n",
    "######################################################################\n",
    "#\n",
    "# 6. 请构建agent:  QuadrotorModel, DDPG, QuadrotorAgent三者嵌套\n",
    "#\n",
    "######################################################################\n",
    "######################################################################\n",
    "model = QuadrotorModel(act_dim)\n",
    "algorithm = DDPG(\n",
    "    model, gamma=GAMMA, tau=TAU, actor_lr=ACTOR_LR, critic_lr=CRITIC_LR)\n",
    "agent = QuadrotorAgent(algorithm, obs_dim, act_dim)\n",
    "\n",
    "\n",
    "# parl库也为DDPG算法内置了ReplayMemory，可直接从 parl.utils 引入使用\n",
    "rpm = ReplayMemory(int(MEMORY_SIZE), obs_dim, act_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06-29 23:49:10 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 1000, Test reward: -120.53889082089563\n",
      "\u001b[32m[06-29 23:51:33 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 10000, Test reward: -111.72708208791828\n",
      "\u001b[32m[06-29 23:51:33 MainThread @machine_info.py:88]\u001b[0m Cannot find available GPU devices, using CPU now.\n",
      "\u001b[32m[06-29 23:56:52 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 20000, Test reward: -330.8010214084134\n",
      "\u001b[32m[06-30 00:02:26 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 30000, Test reward: -625.8783972787609\n",
      "\u001b[32m[06-30 00:07:51 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 40000, Test reward: -309.91523469210307\n",
      "\u001b[32m[06-30 00:13:21 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 50000, Test reward: -121.4161662697833\n",
      "\u001b[32m[06-30 00:18:49 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 60000, Test reward: -96.24692815852477\n",
      "\u001b[32m[06-30 00:24:17 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 70000, Test reward: -58.52923013726935\n",
      "\u001b[32m[06-30 00:29:47 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 80000, Test reward: -63.0164773506825\n",
      "\u001b[32m[06-30 00:35:25 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 90000, Test reward: -89.57179579892457\n",
      "\u001b[32m[06-30 00:40:53 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 100000, Test reward: -85.25345119311172\n",
      "\u001b[32m[06-30 00:46:15 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 110000, Test reward: -109.13287696467467\n",
      "\u001b[32m[06-30 00:51:54 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 120000, Test reward: -65.04097199505108\n",
      "\u001b[32m[06-30 00:57:53 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 130000, Test reward: -78.8370651904109\n",
      "\u001b[32m[06-30 01:03:38 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 140000, Test reward: -88.67195880897512\n",
      "\u001b[32m[06-30 01:09:04 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 150000, Test reward: -76.678334886921\n",
      "\u001b[32m[06-30 01:14:35 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 160000, Test reward: -63.43966657060788\n",
      "\u001b[32m[06-30 01:20:05 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 170000, Test reward: -68.60526797886766\n",
      "\u001b[32m[06-30 01:25:37 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 180000, Test reward: -56.44043034081706\n",
      "\u001b[32m[06-30 01:31:03 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 190000, Test reward: -108.38787876096315\n",
      "\u001b[32m[06-30 01:36:29 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 200000, Test reward: -75.529273064097\n",
      "\u001b[32m[06-30 01:41:57 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 210000, Test reward: -143.31468612967754\n",
      "\u001b[32m[06-30 01:47:25 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 220000, Test reward: -112.78673759535425\n",
      "\u001b[32m[06-30 01:52:53 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 230000, Test reward: -50.11868772232411\n",
      "\u001b[32m[06-30 01:58:18 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 240000, Test reward: -115.5860597287993\n",
      "\u001b[32m[06-30 02:03:41 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 250000, Test reward: -103.57882404526586\n",
      "\u001b[32m[06-30 02:09:39 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 260000, Test reward: -138.2993339564519\n",
      "\u001b[32m[06-30 02:15:37 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 270000, Test reward: -148.42807098787105\n",
      "\u001b[32m[06-30 02:21:35 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 280000, Test reward: -171.15307744864128\n",
      "\u001b[32m[06-30 02:27:36 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 290000, Test reward: -116.96085937519547\n",
      "\u001b[32m[06-30 02:33:37 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 300000, Test reward: -98.77493495767126\n",
      "\u001b[32m[06-30 02:39:45 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 310000, Test reward: -124.17588786203507\n",
      "\u001b[32m[06-30 02:45:53 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 320000, Test reward: -135.38391079556553\n",
      "\u001b[32m[06-30 02:52:02 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 330000, Test reward: -198.03942080921897\n",
      "\u001b[32m[06-30 02:58:11 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 340000, Test reward: -121.73943680080603\n",
      "\u001b[32m[06-30 03:04:16 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 350000, Test reward: -103.53801784469042\n",
      "\u001b[32m[06-30 03:10:22 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 360000, Test reward: -78.20011087207449\n",
      "\u001b[32m[06-30 03:16:27 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 370000, Test reward: -84.77712510791785\n",
      "\u001b[32m[06-30 03:22:32 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 380000, Test reward: -52.37646226658076\n",
      "\u001b[32m[06-30 03:28:35 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 390000, Test reward: -41.61791505012464\n",
      "\u001b[32m[06-30 03:34:36 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 400000, Test reward: -22.73331479707925\n",
      "\u001b[32m[06-30 03:40:35 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 410000, Test reward: -23.25028396299713\n",
      "\u001b[32m[06-30 03:46:34 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 420000, Test reward: -20.82890805745821\n",
      "\u001b[32m[06-30 03:52:33 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 430000, Test reward: -20.452477193686747\n",
      "\u001b[32m[06-30 03:58:34 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 440000, Test reward: -20.378073434449014\n",
      "\u001b[32m[06-30 04:04:24 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 450000, Test reward: -20.858747743264182\n",
      "\u001b[32m[06-30 04:09:58 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 460000, Test reward: -19.615123474513688\n",
      "\u001b[32m[06-30 04:15:28 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 470000, Test reward: -20.913982408063053\n",
      "\u001b[32m[06-30 04:21:15 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 480000, Test reward: -20.471840337258904\n",
      "\u001b[32m[06-30 04:27:16 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 490000, Test reward: -20.14265534891329\n",
      "\u001b[32m[06-30 04:33:17 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 500000, Test reward: -20.30037898161364\n",
      "\u001b[32m[06-30 04:38:56 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 510000, Test reward: -21.025494273739973\n",
      "\u001b[32m[06-30 04:44:26 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 520000, Test reward: -20.89118637970457\n",
      "\u001b[32m[06-30 04:50:00 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 530000, Test reward: -21.058182626386795\n",
      "\u001b[32m[06-30 04:55:36 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 540000, Test reward: -20.63561764583153\n",
      "\u001b[32m[06-30 05:01:09 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 550000, Test reward: -20.584721365969223\n",
      "\u001b[32m[06-30 05:06:41 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 560000, Test reward: -20.268547620005513\n",
      "\u001b[32m[06-30 05:12:11 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 570000, Test reward: -21.14949963238159\n",
      "\u001b[32m[06-30 05:17:38 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 580000, Test reward: -21.100367262225355\n",
      "\u001b[32m[06-30 05:23:03 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 590000, Test reward: -19.78912209554302\n",
      "\u001b[32m[06-30 05:28:29 MainThread @<ipython-input-22-756cb1812c03>:15]\u001b[0m Steps 600000, Test reward: -20.457990373174198\n"
     ]
    }
   ],
   "source": [
    "# 启动训练\n",
    "test_flag = 0\n",
    "total_steps = 0\n",
    "while total_steps < TRAIN_TOTAL_STEPS:\n",
    "    train_reward, steps = run_episode(env, agent, rpm)\n",
    "    total_steps += steps\n",
    "    #logger.info('Steps: {} Reward: {}'.format(total_steps, train_reward)) # 打印训练reward\n",
    "\n",
    "    if total_steps // TEST_EVERY_STEPS >= test_flag: # 每隔一定step数，评估一次模型\n",
    "        while total_steps // TEST_EVERY_STEPS >= test_flag:\n",
    "            test_flag += 1\n",
    " \n",
    "        evaluate_reward = evaluate(env, agent)\n",
    "        logger.info('Steps {}, Test reward: {}'.format(\n",
    "            total_steps, evaluate_reward)) # 打印评估的reward\n",
    "\n",
    "        # 每评估一次，就保存一次模型，以训练的step数命名\n",
    "        ckpt = 'model_dir3/s2[{}]_{}.ckpt'.format(int(evaluate_reward),total_steps)                   #想存不同版本的ckpt文件，可以在此处改目录，一个版本一个目录肯定不会混。\n",
    "        agent.save(ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 验收测评"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " **我的理解是既然是速度控制，那么越接近规定的速度越好，最好的情况就是与规定的速度相同也就是0误差，这可能就是reward要定为很小的负值的意义**\n",
    "\n",
    "* 大家可以看到log信息，reward在35W步的时候得到了最低分-908,训练到75W步的时候基本稳定在-20的水平，说明使用parl框架训练有效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06-30 05:43:50 MainThread @<ipython-input-24-eac67b9de3e5>:12]\u001b[0m Evaluate reward: -20.32238043238665\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "######################################################################\n",
    "#\n",
    "# 7. 请选择你训练的最好的一次模型文件做评估\n",
    "#\n",
    "######################################################################\n",
    "######################################################################\n",
    "ckpt = 'model_dir3/s2[-19]_590000.ckpt'  # 请设置ckpt为你训练中效果最好的一次评估保存的模型文件名称\n",
    "\n",
    "agent.restore(ckpt)\n",
    "evaluate_reward = evaluate(env, agent)\n",
    "logger.info('Evaluate reward: {}'.format(evaluate_reward)) # 打印评估的reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**一个有趣的地方：**\n",
    "* 训练的时候action的GM值取的全局变量，GM= 0.2，但测试的时候我改写了评估程序，令 action = gm * action +(1-gm)* mean_a 。\n",
    "* 这个操作只会对测试产生影响，而不会对rpm产生影响，因为存入rpm的是神经网络的原始输出值。\n",
    "* 评估时每循环一次都改变了gm的值，gm最小取0，最大取1。取gm = 0时，action 失效;取 gm = 1时,mean_a 失效。\n",
    "* 我循环测试了21次，每次gm 值增加0.05 ,即使是gm为0 或者 gm 为1的时候，飞行器都能得到高的reward ，这说明无论是action (具有4个输出维度)，还是 mean_a(只有1个输出维度) 都能独立完成任务。\n",
    "* 这可能就是设置基本值mean_a和浮动值action ，并按一定比例叠加送入到env之后能够提高收敛速度的原因：浮动值和基值均能独立起作用，将其混合之后提高了输出的相对稳定性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "此轮的gm值: 0.0\n",
      "一次评估完成，此时的gm值 0.0 此次的total_reward -20.62000197768242\n",
      "一次评估完成，此时的gm值 0.0 此次的total_reward -22.37969459311374\n",
      "一次评估完成，此时的gm值 0.0 此次的total_reward -19.77147608855221\n",
      "一次评估完成，此时的gm值 0.0 此次的total_reward -20.01903979976777\n",
      "一次评估完成，此时的gm值 0.0 此次的total_reward -22.368639779566113\n",
      "\u001b[32m[06-30 05:44:59 MainThread @<ipython-input-25-2540aa78c845>:41]\u001b[0m Evaluate reward: -21.03177044773645\n",
      "此轮的gm值: 0.05\n",
      "一次评估完成，此时的gm值 0.05 此次的total_reward -19.67669518456917\n",
      "一次评估完成，此时的gm值 0.05 此次的total_reward -20.656513994786216\n",
      "一次评估完成，此时的gm值 0.05 此次的total_reward -21.72788037441839\n",
      "一次评估完成，此时的gm值 0.05 此次的total_reward -21.258179671301026\n",
      "一次评估完成，此时的gm值 0.05 此次的total_reward -20.485445535349516\n",
      "\u001b[32m[06-30 05:45:51 MainThread @<ipython-input-25-2540aa78c845>:41]\u001b[0m Evaluate reward: -20.760942952084864\n",
      "此轮的gm值: 0.1\n",
      "一次评估完成，此时的gm值 0.1 此次的total_reward -20.399884476542073\n",
      "一次评估完成，此时的gm值 0.1 此次的total_reward -21.03938002261719\n",
      "一次评估完成，此时的gm值 0.1 此次的total_reward -20.99360010556331\n",
      "一次评估完成，此时的gm值 0.1 此次的total_reward -20.499585481908895\n",
      "一次评估完成，此时的gm值 0.1 此次的total_reward -19.863236568911955\n",
      "\u001b[32m[06-30 05:46:42 MainThread @<ipython-input-25-2540aa78c845>:41]\u001b[0m Evaluate reward: -20.559137331108683\n",
      "此轮的gm值: 0.15000000000000002\n",
      "一次评估完成，此时的gm值 0.15000000000000002 此次的total_reward -21.808618996787846\n",
      "一次评估完成，此时的gm值 0.15000000000000002 此次的total_reward -19.238800988531832\n",
      "一次评估完成，此时的gm值 0.15000000000000002 此次的total_reward -20.190005719180682\n",
      "一次评估完成，此时的gm值 0.15000000000000002 此次的total_reward -19.76457620049174\n",
      "一次评估完成，此时的gm值 0.15000000000000002 此次的total_reward -19.694225025896003\n",
      "\u001b[32m[06-30 05:47:34 MainThread @<ipython-input-25-2540aa78c845>:41]\u001b[0m Evaluate reward: -20.13924538617762\n",
      "此轮的gm值: 0.2\n",
      "一次评估完成，此时的gm值 0.2 此次的total_reward -22.374912737548428\n",
      "一次评估完成，此时的gm值 0.2 此次的total_reward -22.90306203323522\n",
      "一次评估完成，此时的gm值 0.2 此次的total_reward -19.74012168377092\n",
      "一次评估完成，此时的gm值 0.2 此次的total_reward -20.886770225027586\n",
      "一次评估完成，此时的gm值 0.2 此次的total_reward -19.349587796685185\n",
      "\u001b[32m[06-30 05:48:27 MainThread @<ipython-input-25-2540aa78c845>:41]\u001b[0m Evaluate reward: -21.050890895253467\n",
      "此轮的gm值: 0.25\n",
      "一次评估完成，此时的gm值 0.25 此次的total_reward -20.021906154247535\n",
      "一次评估完成，此时的gm值 0.25 此次的total_reward -19.78038406793601\n",
      "一次评估完成，此时的gm值 0.25 此次的total_reward -19.219831780790003\n",
      "一次评估完成，此时的gm值 0.25 此次的total_reward -21.069489851812477\n",
      "一次评估完成，此时的gm值 0.25 此次的total_reward -20.57133383428054\n",
      "\u001b[32m[06-30 05:49:19 MainThread @<ipython-input-25-2540aa78c845>:41]\u001b[0m Evaluate reward: -20.132589137813312\n",
      "此轮的gm值: 0.30000000000000004\n",
      "一次评估完成，此时的gm值 0.30000000000000004 此次的total_reward -20.392118468203172\n",
      "一次评估完成，此时的gm值 0.30000000000000004 此次的total_reward -19.18985872331927\n",
      "一次评估完成，此时的gm值 0.30000000000000004 此次的total_reward -19.40596016024672\n",
      "一次评估完成，此时的gm值 0.30000000000000004 此次的total_reward -20.543965552295894\n",
      "一次评估完成，此时的gm值 0.30000000000000004 此次的total_reward -20.44632278455755\n",
      "\u001b[32m[06-30 05:50:10 MainThread @<ipython-input-25-2540aa78c845>:41]\u001b[0m Evaluate reward: -19.99564513772452\n",
      "此轮的gm值: 0.35000000000000003\n",
      "一次评估完成，此时的gm值 0.35000000000000003 此次的total_reward -19.067202685550956\n",
      "一次评估完成，此时的gm值 0.35000000000000003 此次的total_reward -20.95651793117369\n",
      "一次评估完成，此时的gm值 0.35000000000000003 此次的total_reward -22.16392023218006\n",
      "一次评估完成，此时的gm值 0.35000000000000003 此次的total_reward -19.277878707570057\n",
      "一次评估完成，此时的gm值 0.35000000000000003 此次的total_reward -19.566421610079455\n",
      "\u001b[32m[06-30 05:51:02 MainThread @<ipython-input-25-2540aa78c845>:41]\u001b[0m Evaluate reward: -20.206388233310843\n",
      "此轮的gm值: 0.4\n",
      "一次评估完成，此时的gm值 0.4 此次的total_reward -22.84965484327381\n",
      "一次评估完成，此时的gm值 0.4 此次的total_reward -21.109139800175438\n",
      "一次评估完成，此时的gm值 0.4 此次的total_reward -23.20164340206529\n",
      "一次评估完成，此时的gm值 0.4 此次的total_reward -18.23532428721546\n",
      "一次评估完成，此时的gm值 0.4 此次的total_reward -21.033936695708395\n",
      "\u001b[32m[06-30 05:51:52 MainThread @<ipython-input-25-2540aa78c845>:41]\u001b[0m Evaluate reward: -21.28593980568768\n",
      "此轮的gm值: 0.45\n",
      "一次评估完成，此时的gm值 0.45 此次的total_reward -20.34611311757071\n",
      "一次评估完成，此时的gm值 0.45 此次的total_reward -20.450562390670687\n",
      "一次评估完成，此时的gm值 0.45 此次的total_reward -20.74115803927544\n",
      "一次评估完成，此时的gm值 0.45 此次的total_reward -20.607904574756596\n",
      "一次评估完成，此时的gm值 0.45 此次的total_reward -21.065526215825017\n",
      "\u001b[32m[06-30 05:52:44 MainThread @<ipython-input-25-2540aa78c845>:41]\u001b[0m Evaluate reward: -20.64225286761969\n",
      "此轮的gm值: 0.5\n",
      "一次评估完成，此时的gm值 0.5 此次的total_reward -18.49509860102338\n",
      "一次评估完成，此时的gm值 0.5 此次的total_reward -19.930216959245573\n",
      "一次评估完成，此时的gm值 0.5 此次的total_reward -22.101754097851813\n",
      "一次评估完成，此时的gm值 0.5 此次的total_reward -21.647687353755288\n",
      "一次评估完成，此时的gm值 0.5 此次的total_reward -18.758142216660794\n",
      "\u001b[32m[06-30 05:53:35 MainThread @<ipython-input-25-2540aa78c845>:41]\u001b[0m Evaluate reward: -20.18657984570737\n",
      "此轮的gm值: 0.55\n",
      "一次评估完成，此时的gm值 0.55 此次的total_reward -19.498057336841\n",
      "一次评估完成，此时的gm值 0.55 此次的total_reward -19.41304425036229\n",
      "一次评估完成，此时的gm值 0.55 此次的total_reward -20.576921017200355\n",
      "一次评估完成，此时的gm值 0.55 此次的total_reward -21.536641331503596\n",
      "一次评估完成，此时的gm值 0.55 此次的total_reward -19.888919019258886\n",
      "\u001b[32m[06-30 05:54:26 MainThread @<ipython-input-25-2540aa78c845>:41]\u001b[0m Evaluate reward: -20.182716591033227\n",
      "此轮的gm值: 0.6000000000000001\n",
      "一次评估完成，此时的gm值 0.6000000000000001 此次的total_reward -20.333188374465113\n",
      "一次评估完成，此时的gm值 0.6000000000000001 此次的total_reward -20.691682494500217\n",
      "一次评估完成，此时的gm值 0.6000000000000001 此次的total_reward -19.335556005151975\n",
      "一次评估完成，此时的gm值 0.6000000000000001 此次的total_reward -20.548796499387965\n",
      "一次评估完成，此时的gm值 0.6000000000000001 此次的total_reward -20.133083912477602\n",
      "\u001b[32m[06-30 05:55:17 MainThread @<ipython-input-25-2540aa78c845>:41]\u001b[0m Evaluate reward: -20.208461457196574\n",
      "此轮的gm值: 0.65\n",
      "一次评估完成，此时的gm值 0.65 此次的total_reward -20.508948447771612\n",
      "一次评估完成，此时的gm值 0.65 此次的total_reward -20.983539860471634\n",
      "一次评估完成，此时的gm值 0.65 此次的total_reward -19.467703837075014\n",
      "一次评估完成，此时的gm值 0.65 此次的total_reward -19.853185513323613\n",
      "一次评估完成，此时的gm值 0.65 此次的total_reward -22.393937538515555\n",
      "\u001b[32m[06-30 05:56:08 MainThread @<ipython-input-25-2540aa78c845>:41]\u001b[0m Evaluate reward: -20.641463039431486\n",
      "此轮的gm值: 0.7000000000000001\n",
      "一次评估完成，此时的gm值 0.7000000000000001 此次的total_reward -20.510889159156143\n",
      "一次评估完成，此时的gm值 0.7000000000000001 此次的total_reward -22.443727470641782\n",
      "一次评估完成，此时的gm值 0.7000000000000001 此次的total_reward -22.584099876191917\n",
      "一次评估完成，此时的gm值 0.7000000000000001 此次的total_reward -21.024148836199664\n",
      "一次评估完成，此时的gm值 0.7000000000000001 此次的total_reward -19.298450521494086\n",
      "\u001b[32m[06-30 05:57:00 MainThread @<ipython-input-25-2540aa78c845>:41]\u001b[0m Evaluate reward: -21.172263172736717\n",
      "此轮的gm值: 0.75\n",
      "一次评估完成，此时的gm值 0.75 此次的total_reward -22.126678421537427\n",
      "一次评估完成，此时的gm值 0.75 此次的total_reward -21.72163092484911\n",
      "一次评估完成，此时的gm值 0.75 此次的total_reward -21.75898965999272\n",
      "一次评估完成，此时的gm值 0.75 此次的total_reward -20.303822368318592\n",
      "一次评估完成，此时的gm值 0.75 此次的total_reward -19.33746372545116\n",
      "\u001b[32m[06-30 05:57:50 MainThread @<ipython-input-25-2540aa78c845>:41]\u001b[0m Evaluate reward: -21.049717020029803\n",
      "此轮的gm值: 0.8\n",
      "一次评估完成，此时的gm值 0.8 此次的total_reward -22.98210882985997\n",
      "一次评估完成，此时的gm值 0.8 此次的total_reward -19.495773941049087\n",
      "一次评估完成，此时的gm值 0.8 此次的total_reward -20.34369369481191\n",
      "一次评估完成，此时的gm值 0.8 此次的total_reward -19.768514438729603\n",
      "一次评估完成，此时的gm值 0.8 此次的total_reward -20.61236038871327\n",
      "\u001b[32m[06-30 05:58:41 MainThread @<ipython-input-25-2540aa78c845>:41]\u001b[0m Evaluate reward: -20.64049025863277\n",
      "此轮的gm值: 0.8500000000000001\n",
      "一次评估完成，此时的gm值 0.8500000000000001 此次的total_reward -22.89273069816531\n",
      "一次评估完成，此时的gm值 0.8500000000000001 此次的total_reward -20.980905501473984\n",
      "一次评估完成，此时的gm值 0.8500000000000001 此次的total_reward -20.809938400832603\n",
      "一次评估完成，此时的gm值 0.8500000000000001 此次的total_reward -21.103611848955463\n",
      "一次评估完成，此时的gm值 0.8500000000000001 此次的total_reward -20.924925805846154\n",
      "\u001b[32m[06-30 05:59:39 MainThread @<ipython-input-25-2540aa78c845>:41]\u001b[0m Evaluate reward: -21.342422451054706\n",
      "此轮的gm值: 0.9\n",
      "一次评估完成，此时的gm值 0.9 此次的total_reward -19.308642171222115\n",
      "一次评估完成，此时的gm值 0.9 此次的total_reward -20.176902867989924\n",
      "一次评估完成，此时的gm值 0.9 此次的total_reward -20.663875281336182\n",
      "一次评估完成，此时的gm值 0.9 此次的total_reward -21.646601153463195\n",
      "一次评估完成，此时的gm值 0.9 此次的total_reward -20.2333672496158\n",
      "\u001b[32m[06-30 06:00:37 MainThread @<ipython-input-25-2540aa78c845>:41]\u001b[0m Evaluate reward: -20.40587774472544\n",
      "此轮的gm值: 0.9500000000000001\n",
      "一次评估完成，此时的gm值 0.9500000000000001 此次的total_reward -20.40908373258784\n",
      "一次评估完成，此时的gm值 0.9500000000000001 此次的total_reward -21.482060036044043\n",
      "一次评估完成，此时的gm值 0.9500000000000001 此次的total_reward -21.448699026591633\n",
      "一次评估完成，此时的gm值 0.9500000000000001 此次的total_reward -20.68380134210911\n",
      "一次评估完成，此时的gm值 0.9500000000000001 此次的total_reward -19.18894895197347\n",
      "\u001b[32m[06-30 06:01:36 MainThread @<ipython-input-25-2540aa78c845>:41]\u001b[0m Evaluate reward: -20.64251861786122\n",
      "此轮的gm值: 1.0\n",
      "一次评估完成，此时的gm值 1.0 此次的total_reward -21.509660732816965\n",
      "一次评估完成，此时的gm值 1.0 此次的total_reward -18.432396528948548\n",
      "一次评估完成，此时的gm值 1.0 此次的total_reward -21.328738890064272\n",
      "一次评估完成，此时的gm值 1.0 此次的total_reward -22.587477135696282\n",
      "一次评估完成，此时的gm值 1.0 此次的total_reward -20.89064472144269\n",
      "\u001b[32m[06-30 06:02:34 MainThread @<ipython-input-25-2540aa78c845>:41]\u001b[0m Evaluate reward: -20.94978360179375\n"
     ]
    }
   ],
   "source": [
    "ckpt = 'model_dir3/s2[-19]_590000.ckpt'  # 请设置ckpt为你训练中效果最好的一次评估保存的模型文件名称\r\n",
    "\r\n",
    "agent.restore(ckpt)\r\n",
    "def evaluate1(env, agent ,gm):\r\n",
    "    \r\n",
    "    eval_reward = []\r\n",
    "    for i in range(5):\r\n",
    "        obs = env.reset()\r\n",
    "        total_reward, steps = 0, 0\r\n",
    "        while True:\r\n",
    "            batch_obs = np.expand_dims(obs, axis=0)\r\n",
    "            action = agent.predict(batch_obs.astype('float32'))\r\n",
    "            ##action[0]  =    action.mean(axis=1)                #加的一行代码，使输出为4个神经元的平均值，此处是之前测试用的，大家也可以试下，\r\n",
    "                                                                 \r\n",
    "\r\n",
    "            action = np.squeeze(action)                      \r\n",
    "            mean_a= action[4]                                     #加的代码，还原输出，目的使输出稳定，原因同上。\r\n",
    "            action = action[0:4]\r\n",
    "            action = gm*action +(1-gm) * mean_a                           #注意此处的gm，用于变电压浮动的控制\r\n",
    "            \r\n",
    "\r\n",
    "            action = np.clip(action, -1.0, 1.0)         #加的一行代码，防止报错\r\n",
    "            action = action_mapping(action, env.action_space.low[0], \r\n",
    "                                    env.action_space.high[0])\r\n",
    "\r\n",
    "            next_obs, reward, done, info = env.step(action)\r\n",
    "\r\n",
    "            obs = next_obs\r\n",
    "            total_reward += reward\r\n",
    "            steps += 1\r\n",
    "\r\n",
    "            if done:\r\n",
    "                break\r\n",
    "        eval_reward.append(total_reward)\r\n",
    "        print(\"一次评估完成，此时的gm值\",gm,\"此次的total_reward\",total_reward)\r\n",
    "    return np.mean(eval_reward)\r\n",
    "for gm in range(21):\r\n",
    "    gm   = 0.05*float(gm)\r\n",
    "    print(\"此轮的gm值:\",gm)\r\n",
    "    evaluate_reward = evaluate1(env, agent,gm)\r\n",
    "    logger.info('Evaluate reward: {}'.format(evaluate_reward)) # 打印评估的reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.6.2 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
